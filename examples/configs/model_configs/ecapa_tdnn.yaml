featurizer:
  feat_in: 80
  filters: [1024,1024,1024,1024,3072]
  kernel_sizes: [5,3,3,3,1]
  dilations: [1,1,1,1,1]
  scale: 8

  decoder:
    feat_in: 3072
    pool_mode: 'attention' #xvector,tap or attention
    emb_sizes: 192

classifier:
  feat_in: 192



